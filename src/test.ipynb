{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df7b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from data_loader import NuplanDataLoader, NuPlanDataset\n",
    "from model import RoadMind\n",
    "from train import train_model\n",
    "from utils import plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e2f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rayan/miniconda3/envs/roadmind_env/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: [0.001]\n",
      "Epoch 1/50 , Train Loss: 37.4051, Val Loss: 12.4525, ADE: 3.9647, FDE: 9.0438\n",
      "Learning rate: [0.001]\n",
      "Epoch 2/50 , Train Loss: 13.7441, Val Loss: 12.7043, ADE: 4.0669, FDE: 9.2247\n",
      "Learning rate: [0.001]\n",
      "Epoch 3/50 , Train Loss: 11.4376, Val Loss: 10.0868, ADE: 3.5807, FDE: 8.4204\n",
      "Learning rate: [0.001]\n",
      "Epoch 4/50 , Train Loss: 10.4154, Val Loss: 8.5716, ADE: 3.1227, FDE: 7.5326\n",
      "Learning rate: [0.001]\n",
      "Epoch 5/50 , Train Loss: 9.0871, Val Loss: 8.9012, ADE: 3.2677, FDE: 7.7904\n",
      "Learning rate: [0.001]\n",
      "Epoch 6/50 , Train Loss: 8.7358, Val Loss: 8.7359, ADE: 3.1907, FDE: 7.6343\n",
      "Learning rate: [0.001]\n",
      "Epoch 7/50 , Train Loss: 7.9381, Val Loss: 7.9143, ADE: 3.0637, FDE: 7.4065\n",
      "Learning rate: [0.001]\n",
      "Epoch 8/50 , Train Loss: 7.1415, Val Loss: 6.6885, ADE: 2.6369, FDE: 6.6644\n",
      "Learning rate: [0.001]\n",
      "Epoch 9/50 , Train Loss: 6.5337, Val Loss: 10.3122, ADE: 3.5700, FDE: 8.3118\n",
      "Learning rate: [0.001]\n",
      "Epoch 10/50 , Train Loss: 6.6135, Val Loss: 9.8929, ADE: 3.4952, FDE: 8.0949\n",
      "Learning rate: [0.001]\n",
      "Epoch 11/50 , Train Loss: 5.8535, Val Loss: 6.1681, ADE: 2.5724, FDE: 6.6218\n",
      "Learning rate: [0.001]\n",
      "Epoch 12/50 , Train Loss: 5.3616, Val Loss: 6.8430, ADE: 2.7388, FDE: 6.8597\n",
      "Learning rate: [0.001]\n",
      "Epoch 13/50 , Train Loss: 4.7353, Val Loss: 6.4174, ADE: 2.6533, FDE: 6.5877\n",
      "Learning rate: [0.001]\n",
      "Epoch 14/50 , Train Loss: 4.5345, Val Loss: 6.6024, ADE: 2.7963, FDE: 7.0710\n",
      "Learning rate: [0.001]\n",
      "Epoch 15/50 , Train Loss: 4.7291, Val Loss: 5.7844, ADE: 2.4580, FDE: 6.2856\n",
      "Learning rate: [0.001]\n",
      "Epoch 16/50 , Train Loss: 4.0740, Val Loss: 5.9796, ADE: 2.4842, FDE: 6.4670\n",
      "Learning rate: [0.001]\n",
      "Epoch 17/50 , Train Loss: 4.0204, Val Loss: 7.3351, ADE: 2.8400, FDE: 6.9533\n",
      "Learning rate: [0.001]\n",
      "Epoch 18/50 , Train Loss: 3.4372, Val Loss: 5.6003, ADE: 2.3696, FDE: 6.1411\n",
      "Learning rate: [0.001]\n",
      "Epoch 19/50 , Train Loss: 3.4191, Val Loss: 5.3576, ADE: 2.3259, FDE: 6.0835\n",
      "Learning rate: [0.001]\n",
      "Epoch 20/50 , Train Loss: 3.0784, Val Loss: 5.5782, ADE: 2.4149, FDE: 6.2758\n",
      "Learning rate: [0.001]\n",
      "Epoch 21/50 , Train Loss: 3.2367, Val Loss: 7.4796, ADE: 2.8307, FDE: 7.0160\n",
      "Learning rate: [0.001]\n",
      "Epoch 22/50 , Train Loss: 3.2439, Val Loss: 5.4737, ADE: 2.3469, FDE: 6.0806\n",
      "Learning rate: [0.001]\n",
      "Epoch 23/50 , Train Loss: 2.9766, Val Loss: 5.5680, ADE: 2.3287, FDE: 6.0477\n",
      "Learning rate: [0.001]\n",
      "Epoch 24/50 , Train Loss: 3.0368, Val Loss: 5.5137, ADE: 2.3863, FDE: 6.1732\n",
      "Learning rate: [0.001]\n",
      "Epoch 25/50 , Train Loss: 2.9342, Val Loss: 5.3494, ADE: 2.2531, FDE: 5.9032\n",
      "Learning rate: [0.001]\n",
      "Epoch 26/50 , Train Loss: 2.8317, Val Loss: 5.7147, ADE: 2.4087, FDE: 6.1023\n",
      "Learning rate: [0.001]\n",
      "Epoch 27/50 , Train Loss: 2.8514, Val Loss: 6.2974, ADE: 2.5124, FDE: 6.3815\n",
      "Learning rate: [0.001]\n",
      "Epoch 28/50 , Train Loss: 2.9181, Val Loss: 5.4918, ADE: 2.3448, FDE: 6.1333\n",
      "Learning rate: [0.001]\n",
      "Epoch 29/50 , Train Loss: 2.8152, Val Loss: 6.5810, ADE: 2.7683, FDE: 6.6753\n",
      "Learning rate: [0.001]\n",
      "Epoch 30/50 , Train Loss: 2.7774, Val Loss: 5.6750, ADE: 2.4095, FDE: 6.1936\n",
      "Learning rate: [0.0002]\n",
      "Epoch 31/50 , Train Loss: 2.6032, Val Loss: 5.7501, ADE: 2.4247, FDE: 6.3496\n",
      "Learning rate: [0.0002]\n",
      "Epoch 32/50 , Train Loss: 2.1541, Val Loss: 4.9627, ADE: 2.1585, FDE: 5.6209\n",
      "Learning rate: [0.0002]\n",
      "Epoch 33/50 , Train Loss: 2.0816, Val Loss: 4.9020, ADE: 2.1609, FDE: 5.6347\n",
      "Learning rate: [0.0002]\n",
      "Epoch 34/50 , Train Loss: 1.9476, Val Loss: 4.8630, ADE: 2.1130, FDE: 5.5689\n",
      "Learning rate: [0.0002]\n",
      "Epoch 35/50 , Train Loss: 1.7905, Val Loss: 4.8074, ADE: 2.1110, FDE: 5.5442\n",
      "Learning rate: [0.0002]\n",
      "Epoch 36/50 , Train Loss: 1.9306, Val Loss: 4.7902, ADE: 2.1097, FDE: 5.5524\n",
      "Learning rate: [0.0002]\n",
      "Epoch 37/50 , Train Loss: 1.8371, Val Loss: 4.9277, ADE: 2.1453, FDE: 5.6759\n",
      "Learning rate: [0.0002]\n",
      "Epoch 38/50 , Train Loss: 1.8714, Val Loss: 4.8287, ADE: 2.1093, FDE: 5.5327\n",
      "Learning rate: [0.0002]\n",
      "Epoch 39/50 , Train Loss: 1.8091, Val Loss: 5.3692, ADE: 2.3279, FDE: 5.8948\n",
      "Learning rate: [0.0002]\n",
      "Epoch 40/50 , Train Loss: 1.7556, Val Loss: 5.0615, ADE: 2.1663, FDE: 5.7162\n",
      "Learning rate: [0.0002]\n",
      "Epoch 41/50 , Train Loss: 1.7913, Val Loss: 5.0889, ADE: 2.2040, FDE: 5.7345\n",
      "Learning rate: [4e-05]\n",
      "Epoch 42/50 , Train Loss: 1.7766, Val Loss: 4.9257, ADE: 2.1394, FDE: 5.5950\n",
      "Learning rate: [4e-05]\n",
      "Epoch 43/50 , Train Loss: 1.5881, Val Loss: 4.8877, ADE: 2.1302, FDE: 5.6037\n",
      "Learning rate: [4e-05]\n",
      "Epoch 44/50 , Train Loss: 1.6314, Val Loss: 4.8830, ADE: 2.1302, FDE: 5.5848\n",
      "Learning rate: [4e-05]\n",
      "Epoch 45/50 , Train Loss: 1.6096, Val Loss: 4.9316, ADE: 2.1428, FDE: 5.6033\n",
      "Learning rate: [4e-05]\n",
      "Epoch 46/50 , Train Loss: 1.6269, Val Loss: 4.9106, ADE: 2.1320, FDE: 5.5782\n",
      "Learning rate: [4e-05]\n",
      "Epoch 47/50 , Train Loss: 1.5973, Val Loss: 4.8665, ADE: 2.1311, FDE: 5.5958\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.2\u001b[39m, patience=\u001b[32m5\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m train_losses, val_losses, val_ade, val_fde = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m                                                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Create a model directory\u001b[39;00m\n\u001b[32m     51\u001b[39m model_path = \u001b[33m'\u001b[39m\u001b[33m../model\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RoadMind/src/train.py:35\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, scheduler, epochs, start_epoch)\u001b[39m\n\u001b[32m     32\u001b[39m     loss.backward()\n\u001b[32m     33\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m avg_train_loss = train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m     38\u001b[39m train_losses.append(avg_train_loss)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "data_dir = '../data'\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "download_data = False  # Set to False if data is already downloaded\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Initialize data loader and download/extract dataset if needed\n",
    "loader = NuplanDataLoader(data_dir=data_dir, download=download_data)\n",
    "\n",
    "# Create data loaders for training, validation and testing\n",
    "data_paths = loader.get_data_paths()\n",
    "train_dataset = NuPlanDataset(data_paths['train'])\n",
    "val_dataset = NuPlanDataset(data_paths['val'])\n",
    "test_dataset = NuPlanDataset(data_paths['test'], testing=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                        num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = RoadMind()\n",
    "\n",
    "# print(summary(model, [(1, 3, 200, 300), (1, 1,), (1, 21, 3)],\n",
    "#         dtypes=[torch.float32, torch.long, torch.float32],\n",
    "#         device=\"cpu\"))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,  weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5)\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, val_ade, val_fde = train_model(model, train_loader, val_loader, optimizer, scheduler,\n",
    "                                                         epochs=epochs)\n",
    "# Create a model directory\n",
    "model_path = '../model'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), model_path + 'roadmind_model.pth')\n",
    "print(f\"Model saved to {model_path}/roadmind_model.pth\")\n",
    "\n",
    "# Plot training and validation losses\n",
    "plot_results(train_losses, val_losses, val_ade, val_fde)\n",
    "print(f\"Loss plot saved to {model_path}/training_metrics.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56052715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unormalize_image(normalized_image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "\n",
    "    # Convert lists to tensors if they aren't already\n",
    "    if not isinstance(mean, torch.Tensor):\n",
    "        mean = torch.tensor(mean, device=normalized_image.device)\n",
    "    if not isinstance(std, torch.Tensor):\n",
    "        std = torch.tensor(std, device=normalized_image.device)\n",
    "    \n",
    "    # Reshape mean and std for proper broadcasting\n",
    "    if normalized_image.dim() == 3:  # (C, H, W)\n",
    "        mean = mean.view(-1, 1, 1)\n",
    "        std = std.view(-1, 1, 1)\n",
    "    elif normalized_image.dim() == 4:  # (B, C, H, W)\n",
    "        mean = mean.view(1, -1, 1, 1)\n",
    "        std = std.view(1, -1, 1, 1)\n",
    "    \n",
    "    # Reverse the normalization: unnormalized = normalized * std + mean\n",
    "    unnormalized = normalized_image * std + mean\n",
    "    \n",
    "    # Scale to [0, 255] range\n",
    "    unnormalized = unnormalized.clamp(0, 1) * 255\n",
    "    \n",
    "    return unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5e1f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m val_batch_zero = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_loader))\n\u001b[32m      2\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m camera = \u001b[43mval_batch_zero\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcamera\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m command = val_batch_zero[\u001b[33m'\u001b[39m\u001b[33mdriving_command\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m      5\u001b[39m history = val_batch_zero[\u001b[33m'\u001b[39m\u001b[33msdc_history_feature\u001b[39m\u001b[33m'\u001b[39m].to(device)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "val_batch_zero = next(iter(val_loader))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "camera = val_batch_zero['camera'].to(device)\n",
    "command = val_batch_zero['driving_command'].to(device)\n",
    "history = val_batch_zero['sdc_history_feature'].to(device)\n",
    "future = val_batch_zero['sdc_future_feature'].to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_future = model(camera, command, history)\n",
    "\n",
    "camera = camera = unormalize_image(camera).cpu().numpy()  # correct unormalization\n",
    "history = history.cpu().numpy()\n",
    "future = future.cpu().numpy()\n",
    "pred_future = pred_future.cpu().numpy()\n",
    "k=4\n",
    "selected_indices = random.choices(np.arange(len(camera)), k=k)\n",
    "# plot the camera view of current step for the k examples\n",
    "fig, axis = plt.subplots(1, k, figsize=(4*k, 4))\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    axis[i].imshow(camera[idx].transpose(1, 2, 0)/255)\n",
    "    axis[i].axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# plot the past and future trajectory of the vehicle\n",
    "fig, axis = plt.subplots(1, k, figsize=(4*k, 4))\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    axis[i].plot(history[idx, :, 0], history[idx, :, 1], \"o-\", color=\"gold\", label=\"Past\")\n",
    "    axis[i].plot(future[idx, :, 0], future[idx, :, 1], \"o-\", color=\"green\", label=\"Future\")\n",
    "    axis[i].plot(pred_future[idx, :, 0], pred_future[idx, :, 1], \"o-\", color=\"red\", label=\"Predicted\")\n",
    "    axis[i].legend()\n",
    "    axis[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9cb30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_xy: (1000, 121)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "all_plans = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        camera = batch['camera'].to(device)\n",
    "        driving_command = batch['driving_command'].to(device)\n",
    "        history = batch['sdc_history_feature'].to(device)\n",
    "         \n",
    "        pred_future = model(camera, driving_command, history)\n",
    "        all_plans.append(pred_future.cpu().numpy()[..., :2])\n",
    "all_plans = np.concatenate(all_plans, axis=0)\n",
    "\n",
    "# Now save the plans as a csv file\n",
    "pred_xy = all_plans[..., :2]  # shape: (total_samples, T, 2)\n",
    "\n",
    "# Flatten to (total_samples, T*2)\n",
    "total_samples, T, D = pred_xy.shape\n",
    "pred_xy_flat = pred_xy.reshape(total_samples, T * D)\n",
    "\n",
    "# Build a DataFrame with an ID column\n",
    "ids = np.arange(total_samples)\n",
    "df_xy = pd.DataFrame(pred_xy_flat)\n",
    "df_xy.insert(0, \"id\", ids)\n",
    "\n",
    "# Column names: id, x_1, y_1, x_2, y_2, ..., x_T, y_T\n",
    "new_col_names = [\"id\"]\n",
    "for t in range(1, T + 1):\n",
    "    new_col_names.append(f\"x_{t}\")\n",
    "    new_col_names.append(f\"y_{t}\")\n",
    "df_xy.columns = new_col_names\n",
    "\n",
    "# Save to CSV\n",
    "submission_dir = '../submission'\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "df_xy.to_csv(submission_dir + \"/submission_test.csv\", index=False)\n",
    "\n",
    "print(f\"Shape of df_xy: {df_xy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275a81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roadmind_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
